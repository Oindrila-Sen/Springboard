{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Instacart_logo_small.png\" alt=\"Instacart\" style=\"width: 100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommend products to Instacart Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the English dictionary, **Recommendation** means a suggestion/advice that something is good.\n",
    "<br><br>In today's world of multiple choices, customers are often confused with options. Browsing through hundreds of products makes shopping a challenging and time-consuming experience. \n",
    "<br>So, how about we login to a shopping site and see 10 recommended products tailored to our taste? Just **Add** those to cart, **checkout** and done! This is what a recommendation engine does! Thus a ***Recommendation Engine*** is a Machine Learning Technique that let us predict what a user may or may not like among a list of given items. \n",
    "<br><br> Here, let's use the dataset provided by **Instacart** to build a recommendation engine and Evaluate how our recommendation works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from joblib import dump, load\n",
    "from sklearn import metrics\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order datasets\n",
    "df_order_products_prior = pd.read_csv(\"instacart_2017_05_01/order_products_prior.csv\")\n",
    "df_order_products_train = pd.read_csv(\"instacart_2017_05_01/order_products_train.csv\")\n",
    "df_orders = pd.read_csv(\"instacart_2017_05_01/orders.csv\") \n",
    "# Products\n",
    "df_products = pd.read_csv(\"instacart_2017_05_01/products.csv\")\n",
    "# Departments\n",
    "df_departments = pd.read_csv(\"instacart_2017_05_01/departments.csv\")\n",
    "# Merge prior orders and products\n",
    "# Merge prior orders and products\n",
    "df_merged_order_products_prior = pd.merge(df_order_products_prior, df_products, on=\"product_id\", how=\"left\")\n",
    "df_merged_order_products_prior = pd.merge(df_merged_order_products_prior, df_departments, on=\"department_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>33120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Organic Egg Whites</td>\n",
       "      <td>86</td>\n",
       "      <td>16</td>\n",
       "      <td>dairy eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>28985</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Michigan Organic Kale</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>produce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9327</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Garlic Powder</td>\n",
       "      <td>104</td>\n",
       "      <td>13</td>\n",
       "      <td>pantry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>45918</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Coconut Butter</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>pantry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>30035</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Natural Sweetener</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>pantry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  product_id  add_to_cart_order  reordered           product_name  \\\n",
       "0         2       33120                  1          1     Organic Egg Whites   \n",
       "1         2       28985                  2          1  Michigan Organic Kale   \n",
       "2         2        9327                  3          0          Garlic Powder   \n",
       "3         2       45918                  4          1         Coconut Butter   \n",
       "4         2       30035                  5          0      Natural Sweetener   \n",
       "\n",
       "   aisle_id  department_id  department  \n",
       "0        86             16  dairy eggs  \n",
       "1        83              4     produce  \n",
       "2       104             13      pantry  \n",
       "3        19             13      pantry  \n",
       "4        17             13      pantry  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_order_products_prior.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Basic Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Trending Products at Instacart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New to Instacart and not sure what to order? Let's check out what other customers are buying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_popular(k, df_items):\n",
    "    \"\"\"\n",
    "    Returns the `k` most popular products based on purchase count in the dataset\n",
    "    \"\"\"\n",
    "    popular_products = list(df_items[\"product_name\"].value_counts().head(k).index)\n",
    "    return popular_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Banana',\n",
       " 'Bag of Organic Bananas',\n",
       " 'Organic Strawberries',\n",
       " 'Organic Baby Spinach',\n",
       " 'Organic Hass Avocado',\n",
       " 'Organic Avocado',\n",
       " 'Large Lemon',\n",
       " 'Strawberries',\n",
       " 'Limes',\n",
       " 'Organic Whole Milk']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_popular(10,df_merged_order_products_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recommendations from the Departments that the Customers are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_popular_dept_items(k, dept_id, df_items):\n",
    "    \"\"\"\n",
    "    Returns the `k` most popular products from the Dept that is passed as a parameter\n",
    "    \n",
    "    k       : No. Of Recommendations\n",
    "    dept_id : Pass in the Department Id that you are looking details for\n",
    "    df_items: Pass in the Dataframe with Details\n",
    "    \n",
    "    \"\"\"\n",
    "    dept_popular_products = list(df_items[df_items.department_id == dept_id][\"product_name\"].value_counts().head(k).index)\n",
    "    return dept_popular_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see what are the 10 popular products from Department_id = 6 (International)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Organic Sea Salt Roasted Seaweed Snacks',\n",
       " 'Taco Seasoning',\n",
       " 'New Mexico Taco Skillet Sauce For Chicken',\n",
       " 'Sriracha Chili Sauce',\n",
       " 'Original Roasted Seaweed Snacks',\n",
       " 'Coconut Milk',\n",
       " 'Organic Spicy Taco Seasoning',\n",
       " 'Sriracha Hot Chili Sauce',\n",
       " 'Roasted Sesame Seaweed Snacks',\n",
       " 'Sliced Water Chestnuts']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_popular_dept_items(10,6,df_merged_order_products_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recommended Items that other Customers Often Buy Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reordered_prods(k, df_items):\n",
    "    \"\"\"\n",
    "    Returns the `k` most popular products from the Dept that is passed as a parameter\n",
    "    \n",
    "    k       : No. Of Recommendations\n",
    "    df_items: Pass in the Dataframe with Details\n",
    "    \n",
    "    \"\"\"\n",
    "    reordered_products = list(df_items[\"product_name\"].value_counts().head(10).index)\n",
    "    return reordered_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Banana',\n",
       " 'Bag of Organic Bananas',\n",
       " 'Organic Strawberries',\n",
       " 'Organic Baby Spinach',\n",
       " 'Organic Hass Avocado',\n",
       " 'Organic Avocado',\n",
       " 'Organic Whole Milk',\n",
       " 'Large Lemon',\n",
       " 'Organic Raspberries',\n",
       " 'Strawberries']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_merged_order_products_prior[df_merged_order_products_prior.reordered ==1]\n",
    "get_reordered_prods(10, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Shop Unique Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_unique(k, df_items):\n",
    "    \"\"\"\n",
    "    Returns the `k` unique products based on purchase count in the dataset\n",
    "    \"\"\"\n",
    "    unique_products = list(df_items[\"product_name\"].value_counts().tail(k).index)\n",
    "    return unique_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Max White With Polishing Star Soft Toothbrush',\n",
       " 'Salsa, Black Bean',\n",
       " 'Flame Roasted Red Peppers Spreadable Cheese',\n",
       " 'Original Salted Caramel Protein Energy Bar',\n",
       " 'Seasoned Southern Style Red Beans And Rice',\n",
       " \"Frittata, Farmer's Market\",\n",
       " 'Drink Distinct All Natural Soda Pineapple Coconut & Nutmeg',\n",
       " 'Chelated Magnesium 250 Mg Gluten Free',\n",
       " 'Hot Oatmeal Multigrain Raisin',\n",
       " 'Zingz Queso Fundido Baked Snack Crackers']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_unique(10,df_merged_order_products_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Build Reommendation Engine using Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collborative Filtering:** A technique used for Recommendations by collecting user’s past behaviors (items previously purchased or reordered) as well as similar decisions made by other users.\n",
    "<br><br> **Assumption:** If a person A has the same opinion as a person B on a product, A is more likely to have B's opinion on a different product than that of a randomly chosen person. Hence, These predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an average (non-specific) score for each item of interest, for example based on its number of times bought/ reordered etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use the Prior Data for training a Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset Based on Reordered Quantity\n",
    "data = pd.merge(df_orders.loc[df_orders.eval_set == \"prior\"], df_order_products_prior[[\"order_id\", \"product_id\",\"reordered\"]], on=\"order_id\")\n",
    "data = data.dropna()\n",
    "data = data.copy()\n",
    "data = data[[\"user_id\", \"product_id\",\"reordered\"]]\n",
    "data = data.groupby([\"user_id\", \"product_id\"])['reordered'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reordered</th>\n",
       "      <th>prev_product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>9</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10249</td>\n",
       "      <td>8</td>\n",
       "      <td>10249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10317</td>\n",
       "      <td>0</td>\n",
       "      <td>10317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12416</td>\n",
       "      <td>9</td>\n",
       "      <td>12416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13021</td>\n",
       "      <td>2</td>\n",
       "      <td>13021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>13165</td>\n",
       "      <td>1</td>\n",
       "      <td>13165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>17110</td>\n",
       "      <td>0</td>\n",
       "      <td>17110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>25119</td>\n",
       "      <td>7</td>\n",
       "      <td>25119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>26070</td>\n",
       "      <td>1</td>\n",
       "      <td>26070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>26387</td>\n",
       "      <td>1</td>\n",
       "      <td>26387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  product_id  reordered  prev_product_id\n",
       "0        0         195          9              195\n",
       "1        0       10249          8            10249\n",
       "2        0       10317          0            10317\n",
       "3        0       12416          9            12416\n",
       "4        0       13021          2            13021\n",
       "5        0       13165          1            13165\n",
       "6        0       17110          0            17110\n",
       "7        0       25119          7            25119\n",
       "8        0       26070          1            26070\n",
       "9        0       26387          1            26387"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert product names into numerical IDs\n",
    "c = data.product_id.astype('category')\n",
    "d = dict(enumerate(c.cat.categories))\n",
    "#print (d)\n",
    "data[\"user_id\"] = data[\"user_id\"].astype(\"category\").cat.codes\n",
    "data[\"product_id\"] = data[\"product_id\"].astype(\"category\").cat.codes\n",
    "data.head()\n",
    "data['prev_product_id'] = data['product_id'].map(d)\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implicit library expects data as a item-user matrix. So, we create two matricies, one for fitting the model (item-user) and one for recommendations (user-item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_user matrix\n",
    "sparse_item_user = sparse.csr_matrix((data['reordered'].astype(float), (data['product_id'], data['user_id'])))\n",
    "# user_item matrix\n",
    "sparse_user_item = sparse.csr_matrix((data['reordered'].astype(float), (data['user_id'], data['product_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep the Data\n",
    "item_user_matrix = sparse.coo_matrix((data[\"reordered\"],\n",
    "                                            (data[\"product_id\"],\n",
    "                                             data[\"user_id\"])))\n",
    "# Contruct a sparse matrix for our users and items containing number of reordered\n",
    "item_user_matrix = item_user_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the sparsity of the matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.94798258401877"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_size = item_user_matrix.shape[0]*item_user_matrix.shape[1] # Number of possible interactions in the matrix\n",
    "num_purchases = len(item_user_matrix.nonzero()[0]) # Number of items interacted with\n",
    "sparsity = 100*(1 - (num_purchases/matrix_size))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Testing purpose, we will hide a certain percentage of the user/item interactions from the model during the training phase, chosen at random. Then, check during the test phase how many of the items that were recommended the user actually ended up purchasing in the end. \n",
    "\n",
    "Our test set is an exact copy of the original data. The training set, however, will mask a random percentage of user/item interactions and act as if the user never purchased the item (making it a sparse entry with a zero). We then check in the test set which items were recommended to the user that they ended up actually purchasing. If the users frequently ended up purchasing the items most recommended to them by the system, we can conclude the system seems to be working.\n",
    "\n",
    "As an additional check, we can compare our system to simply recommending the most popular items to every user (beating popularity is a bit difficult). This will be our baseline.\n",
    "\n",
    "This method of testing isn’t necessarily the “correct” answer, because it depends on how you want to use the recommender system. However, it is a practical way of testing performance I will use for this example.\n",
    "\n",
    "Now that we have a plan on how to separate our training and testing sets, let’s create a function that can do this for us. We will also import the random library and set a seed so that you will see the same results as I did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(data, pct_test = 0.2):\n",
    "    '''\n",
    "    This function will take in the original user-item matrix and \"mask\" a percentage of the original ratings where a\n",
    "    user-item interaction has taken place for use as a test set. The test set will contain all of the original ratings, \n",
    "    while the training set replaces the specified percentage of them with a zero in the original ratings matrix. \n",
    "    \n",
    "    '''\n",
    "    test_set = data.copy() # Make a copy of the original set to be the test set. \n",
    "    test_set[test_set != 0] = 1 # Store the test set as a binary preference matrix\n",
    "    training_set = data.copy() # Make a copy of the original data we can alter as our training set. \n",
    "    nonzero_inds = training_set.nonzero() # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # Zip these pairs together of user,item index into list\n",
    "    random.seed(0) # Set the random seed to zero for reproducibility\n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples) # Sample a random number of user-item pairs without replacement\n",
    "    user_inds = [index[0] for index in samples] # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples] # Get the item column indices\n",
    "    training_set[user_inds, item_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set.eliminate_zeros() # Get rid of zeros in sparse array storage after update to save space\n",
    "    return training_set, test_set, list(set(user_inds)) # Output the unique list of user rows that were altered  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, item_users_altered = make_train_test(item_user_matrix, pct_test = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Implicit Matrix Factorization using ALS (Alternating Least Squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implicit Data:** The data that we gather from the users behaviour, with no ratings or specific actions are Implicit Data. For example, with star ratings we know that a 1 means the user did not like the item and a 5 that they really loved it. But here, we do not have a rating for any item. So, we need to build a Recommendation Engine based on what items a customer purchased and how many times (Reordered).\n",
    "\n",
    "**Alternating Least Squares (ALS):** \n",
    "Alternating Least Squares (ALS) is a the model we’ll use to fit our data and find similarities. ALS uses Matrix Factorization method for recommendations.\n",
    "\n",
    "**Matrix Factorization:** The idea is to take a large matrix and factor it into some smaller representation of the original matrix.  \n",
    "<br>We have an original matrix R of size **MxN**, where M is the number of users and N is the number of items. This matrix is quite sparse, since most users only interact with a few items each. We can factorize this matrix into two separate smaller matrices: one with dimensions **MxK** which will be our latent user feature vectors for each user (U) and a second with dimensions **KxN**, which will have our latent item feature vectors for each item (V). Multiplying these two feature matrices together approximates the original matrix, but now we have two matrices that are dense including a number of latent features K for each of our items and users.We calculate U and V so that their product approximates R as closely as possible: **R ≈ U x V.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_matrix(input_matrix, alpha):\n",
    "    \"\"\"\n",
    "    Given a utility matrix,\n",
    "    Returns the given matrix converted to a confidence matrix\n",
    "    \"\"\"\n",
    "    return (input_matrix * alpha).astype(\"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit_als(input_matrix, **kwargs):\n",
    "    \"\"\"\n",
    "    Given the utility matrix and model parameters,\n",
    "    Builds models and writes it to disk \n",
    "    Args:\n",
    "    sparse_data (csr_matrix): Our sparse user-by-item matrix\n",
    "    alpha_val (int)         : The rate in which we'll increase our confidence in a preference with more interactions.\n",
    "    \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building ALS model with alpha: {} \".format(kwargs[\"alpha\"]))\n",
    "    model = AlternatingLeastSquares(factors=20, regularization=0.1, iterations=40)\n",
    "    #model.approximate_similar_items = True\n",
    "    \n",
    "    # Calculate the confidence by multiplying it by alpha value.\n",
    "    data_conf = confidence_matrix(input_matrix, kwargs[\"alpha\"])\n",
    "    \n",
    "    model.fit(data_conf)\n",
    "\n",
    "    # Save model to disk\n",
    "    filename = 'baseline_model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    print(\"Completed in {:.2f}s\".format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the Function to build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ALS model with alpha: 15 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6abf3a62860436081d570f875189ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed in 277.98s\n"
     ]
    }
   ],
   "source": [
    "# Specify model params and build it\n",
    "## Alpha's in the range [10, 50] with a step size of 5 were tried. alpha = 15 was found to have the best overall \n",
    "## recall value. \n",
    "model_params = {\"alpha\": 15} \n",
    "\n",
    "# Build the Model\n",
    "implicit_als(df_train, **model_params)\n",
    "\n",
    "als_model = pickle.load(open('baseline_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similar Items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_items(model,item_id,n_similar):\n",
    "    \"\"\"\n",
    "    Given an item, prints similar items\n",
    "    \"\"\"\n",
    "    product_id = []\n",
    "    product_name = []\n",
    "    scores = []\n",
    "    \n",
    "    similar =  model.similar_items(item_id, n_similar)\n",
    "    for item in similar:\n",
    "        idx, score = item\n",
    "\n",
    "        product_id.append(idx)\n",
    "        scores.append(score)\n",
    "        product_name.append(df_products.product_name.loc[df_products.product_id==idx].iloc[0])\n",
    "    print(\"Similar Items to Item: \",df_products.product_name.loc[df_products.product_id==item_id].iloc[0])\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print(list(product_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Items to Item:  Organic Egg Whites\n",
      "----------------------------------------------------------------------\n",
      "['Organic Egg Whites', 'Organic Grass fed Creamline Yogurt Vanilla', 'Gluten FreeBread & Pizza Crust Mix', 'Pumpkin Pie', 'T/Gel Original Formula Therapeutic Shampoo', 'Original', 'Just Real Fruits & Veggies Snack Apple, Green Pea, Pineapple', 'Boneless Fried Chicken & Waffles', 'Salmon Creations Lemon Dill Salmon', 'Mocha Iced Coffee']\n"
     ]
    }
   ],
   "source": [
    "find_similar_items(als_model,33120,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar Items return some heathy options. Let's Evaluate the performance of the recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(predictions, test):\n",
    "    '''\n",
    "    This simple function will output the area under the curve using sklearn's metrics. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    - predictions: your prediction output\n",
    "    \n",
    "    - test: the actual target result you are comparing to\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    - AUC (area under the Receiver Operating Characterisic curve)\n",
    "    '''\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, utilizing the above function inside of a second function, we will calculate the AUC for each user in our training set that had at least one item masked. It should also calculate AUC for the most popular items for our users to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_auc(training_set, altered_users, predictions, test_set):\n",
    "    '''\n",
    "    This function will calculate the mean AUC by user for any user that had their user-item matrix altered. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    training_set - The training set resulting from make_train, where a certain percentage of the original\n",
    "    user/item interactions are reset to zero to hide them from the model \n",
    "    \n",
    "    predictions - The matrix of your predicted ratings for each user/item pair as output from the implicit MF.\n",
    "    These should be stored in a list, with user vectors as item zero and item vectors as item one. \n",
    "    \n",
    "    altered_users - The indices of the users where at least one user/item pair was altered from make_train function\n",
    "    \n",
    "    test_set - The test set constucted earlier from make_train function\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    The mean AUC (area under the Receiver Operator Characteristic curve) of the test set only on user-item interactions\n",
    "    there were originally zero to test ranking ability in addition to the most popular items as a benchmark.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    store_auc = [] # An empty list to store the AUC for each user that had an item removed from the training set\n",
    "    popularity_auc = [] # To store popular AUC scores\n",
    "    pop_items = np.array(test_set.sum(axis = 0)).reshape(-1) # Get sum of item iteractions to find most popular\n",
    "    item_vecs = predictions[1]\n",
    "    for user in altered_users: # Iterate through each user that had an item altered\n",
    "        training_row = training_set[user,:].toarray().reshape(-1) # Get the training set row\n",
    "        zero_inds = np.where(training_row == 0) # Find where the interaction had not yet occurred\n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        user_vec = predictions[0][user,:]\n",
    "        pred = user_vec.dot(item_vecs).toarray()[0,zero_inds].reshape(-1)\n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set[user,:].toarray()[0,zero_inds].reshape(-1) \n",
    "        # Select the binarized yes/no interaction pairs from the original full data\n",
    "        # that align with the same pairs in training \n",
    "        pop = pop_items[zero_inds] # Get the item popularity for our chosen items\n",
    "        store_auc.append(auc_score(pred, actual)) # Calculate AUC for the given user and store\n",
    "        popularity_auc.append(auc_score(pop, actual)) # Calculate AUC using most popular and score\n",
    "    # End users iteration\n",
    "    \n",
    "    return float('%.3f'%np.mean(store_auc)), float('%.3f'%np.mean(popularity_auc))  \n",
    "   # Return the mean AUC rounded to three decimal places for both test and popularity benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how our recommender system is doing. To use this function, we will need to transform our output from the ALS function to csr_matrix format and transpose the item vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:implicit:This method is deprecated. Please use the AlternatingLeastSquares class instead\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cc6f4868b4405ab25ba6632d4397a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 15\n",
    "user_vecs, item_vecs = implicit.alternating_least_squares((df_train*alpha).astype('double'), \n",
    "                                                          factors=20, \n",
    "                                                          regularization = 0.1, \n",
    "                                                          iterations = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.851, 0.769)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_mean_auc(df_train, item_users_altered, \n",
    "              [sparse.csr_matrix(user_vecs), sparse.csr_matrix(item_vecs.T)], df_test)\n",
    "# AUC for our recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that our recommender system beats popularity. The Recommender system had a mean AUC of **0.85**, while the popular item benchmark had a lower AUC of **0.77**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recommendations for Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s examine the recommendations given to a particular user and see if the user has purchased any of the recommended products!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_items(model,user_id,n_count):\n",
    "    \"\"\"\n",
    "    Get Recommendations for users\n",
    "    \"\"\"\n",
    "    # Recommend items for a user 49676\n",
    "    recommendations = model.recommend(user_id, sparse_user_item, N = n_count)\n",
    "    product_id = []\n",
    "    product_name = []\n",
    "    scores = []\n",
    "    print(\"Recommended Prodcucts for user_id \",user_id)\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    for item in recommendations:\n",
    "        idx, score = item\n",
    "        #print(item)\n",
    "        pid = data.prev_product_id.loc[data.product_id == idx].iloc[0]\n",
    "        ##product_id.append(df_train.product_id.loc[df_train.product_id == idx].iloc[0])\n",
    "        product_name.append(df_products.product_name.loc[df_products.product_id==pid].iloc[0])\n",
    "        #scores.append(score)\n",
    "    print (product_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Recommendations for User#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Prodcucts for user_id  1\n",
      "----------------------------------------------------------------------\n",
      "['Light Strawberry Blueberry Yogurt', 'All-Seasons Salt', 'Indian Hemp & Haitian Vetiver Deodorant', 'Diet Mountain Dew Mini Cans', 'Moisturizing Conditioner Aloe Vera + Macadamia Oil', 'Turkey Stew With Barley & Carrots Natural Dog Food', \"Chocolate Builder's Protein Bar\", 'Franks, Beef, Deli Style, Bun Size', 'SmartBlend Chicken & Rice Formula Adult Dry Dog Food', 'Milk, Low Fat, 1% Milkfat']\n"
     ]
    }
   ],
   "source": [
    "# Let's test for user# 1\n",
    "get_recommendations_items(als_model,1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the products the User Actually Bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_arr = np.array(data.user_id.unique()) # Array of customer IDs from the ratings matrix\n",
    "products_arr = np.array(data.product_id.unique()) # Array of product IDs from the ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_buy(user_id):\n",
    "    \"\"\"\n",
    "    Get the product List the Customer actually bought\n",
    "    \"\"\"\n",
    "    act_products = []\n",
    "    purchased_ind = list(data[data.user_id ==user_id]['prev_product_id'].unique())\n",
    "    print(\"Actual Prodcucts for user_id \",user_id)\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    for pid in purchased_ind:\n",
    "        #id = data.prev_product_id.loc[data.product_id == purchased_ind].iloc[0]\n",
    "        act_products.extend(df_products[df_products.product_id == pid]['product_name'])\n",
    "\n",
    "    print( list(act_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the products actually bought by User#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Prodcucts for user_id  1\n",
      "----------------------------------------------------------------------\n",
      "['Fresh Breath Oral Rinse Mild Mint', 'Nutter Butter Cookie Bites Go-Pak', 'Beef Empanadas', 'Casareccia', 'Hearth Baked Style Twin French Bread', 'Frizz Ease Original Formula Medium To Coarse Frizzy Hair Serum', \"Children's Allergy Relief Chewable Grape Tablets\", 'Chocolate Almond Fudge Ice Cream', '2 in 1  Cavity and Enamel Protection Strawberry Flavor Kids Toothpaste', 'Organic Ginger Beet Kraut', 'Apple Pie Spice', 'Classic Cheddar Popcorn', 'Whoppers Robin Eggs', 'Hazelnut Toffee Dark Chocolate Bar', 'Karamel Sutra Core Ice Cream', 'Decaffeinated Dark Italian Roast Ground Coffee', 'Mexican-Style Ground Spiked Eggnog Dark Chocolate', '100% Grape Juice Concentrate', 'Stock-In-A-Box', 'Salt and Vinegar Chips', \"Lori's Lemon Tea\", 'Natural Dog Food Turkey & Sweet Potato Formula', 'Cranberry Zero Soda', 'Extra Strength Glacier Mint Drops With Natural Menthol', 'Ultra Soft & Strong Bath Tissue', 'Sea Salt Garden Veggie Straws', 'Wicked Fresh! Spearmint Ice Toothpaste', 'Unbleached Self Rising Flour', 'Bryonia 30 C - 80 CT', 'Quartered Artichoke Hearts', 'Alaskan Sockeye Smoked Salmon', 'Bacon Scramble', 'Stewed Italian Recipe Tomatoes', 'Silver Sugar Cupcake Gems', 'Double Concentrated Tomato Paste', 'Healthy Request Condensed Homestyle Chicken Noodle Soup', 'Honey Roasted Almonds', 'Smoothing Repair Body Lotion', 'Lime Juice Squeezed from Real Limes', 'Belvita Soft Baked Mixed Berry', 'Cold Brew Double Expresso', 'Hot Apple Cider Tea Herbs Of Autumn', 'Chocolate Cherry Almond Bites', 'Organic Strawberries', 'Organic Dark Hot Chocolate', 'Men+Care Clean Comfort Antiperspirant Deodorant', 'Cookies, Family Recipe, Double Chocolate Chip', 'Gallon Size Single Slide Storage Bags', 'Decorating Cupcake Icing, Petal Pink', 'Pretzels', 'Glucosamine Chondroitin Complex', 'Gentle Glide 360 Unscented Regular Tampons', 'Deluxe French Salad Dressing', 'Garlic & Green Onion Teriyaki Sauce', 'Unsweetened Almondmilk', 'Rapid Rise Fast Acting Instant Yeast', 'Organic Papaya Poppy Seed Dressing', 'Roasted Red Pepper Dressing', 'ProMist Spray Microfiber Mop', 'Traditional Creamy Tomato with Penne', 'Mozzarella String Cheese Sticks, Light Low-Moisture Part Skim', 'Double Spice Chai Black Tea', 'Ground Thyme', 'Sprouted Multigrain Bread', 'Green Moroccan Mint Tea', '93/7 Ground Beef', 'Carrots, Bag', 'Chocolate Co. Organic Dark Chocolate Sea Salt & Coconut', 'Geranium Scent Laundry Detergent', 'Deep Dish Singles Cheese Pizza', 'Classic Clean Conditioner', 'Hamburger Dill Chips Pickles', 'Fat Free Beef Broth', 'Calm Banana Pumpkin Coconut Blackberry Vanilla Passion Flower Chia', 'Spring Mint Mouthwash', 'Chocolate Chip Macadamia Love Cookies', 'Organic Caesar Salad', 'Focaccia With Olive Oil & Rosemary', 'Extra Hot Cocktail Sauce', 'Free & Clear Dishwasher Rinse Aid', 'Sour Sliced French Enriched Bread', 'Pizza Crust Mix', 'Sweet Tea W/Real Sugar Iced Tea', 'Lemon Meringue Pie', 'Russet Potato Bag', 'Non-Drowsy Liquid Allergies Grape', 'Brown Omega 3 Large Grade A Eggs', 'Freeze Dried Apple & Mango Tiny Fruits', 'Sour Gold Bears', 'Whole Grain Brown Rice Thin Cakes', 'Mini Chocolate Chip Waffles', 'Lite Energy Drink', 'All Natural Seafood Fish Fry Breading Mix', 'Bueno', \"Organic Honey'd Corn Flakes Cereal Gluten Free\", 'Triple Soothing Action Mentho Lyptus Cough Drops Bonus Bag', 'Fresh Collection Denali Scent Deodorant', 'Lemon Supreme Frosting', 'Japaleno Green Salsa Hot']\n"
     ]
    }
   ],
   "source": [
    "get_actual_buy(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it turns out that the user#1 bought some Food Items and thus some different food items are recommended by our Model. Interestingly, User#1 bought a Deodorant('Fresh Collection Denali Scent Deodorant') and one('Indian Hemp & Haitian Vetiver Deodorant') is recommended.\n",
    "\n",
    "Also Dog Food('Natural Dog Food Turkey & Sweet Potato Formula') was actually bought by the user and \"SmartBlend Chicken & Rice Formula Adult Dry Dog Food\" is recommended. \n",
    "\n",
    "While shopping, I **do not** want to see the Items that I usually buy as \"Recommnedations\". I would like to see some **similar** items which are bought by the other customers and catch my attention. So, it looks like our **Recommendation Engine** is working just fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
